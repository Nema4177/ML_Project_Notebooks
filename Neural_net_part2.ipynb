{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3BVh0ffV-5uU"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5hjustDOxCjK"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSgRWvl1EBSN"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('All_numbers.csv')\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-ZuCXFU5EtA2"
   },
   "outputs": [],
   "source": [
    "# Replace nan with most occuring\n",
    "train_df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvUaLwEqEuqB"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUL4s18_E1vK"
   },
   "outputs": [],
   "source": [
    "## Create our Text Vectorizer to index our vocabulary based on the train samples \n",
    "import tensorflow as tf\n",
    "from keras.layers import TextVectorization\n",
    "\n",
    "def vectorize_data(data):\n",
    "    vectorizer = TextVectorization(max_tokens=10000, output_sequence_length=1)\n",
    "    text_ds = tf.data.Dataset.from_tensor_slices(data).batch(128) ## Read batches of 128 samples\n",
    "    vectorizer.adapt(text_ds)\n",
    "    ## Create a map to get the unique list of the vocabulary\n",
    "    voc = vectorizer.get_vocabulary()\n",
    "    word_index = dict(zip(voc, range(len(voc))))\n",
    "    \n",
    "    x_train_data = vectorizer(np.array([[s] for s in data])).numpy()\n",
    "    return x_train_data\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jq_9nO-KHyqn"
   },
   "outputs": [],
   "source": [
    "list_strings = [\"City\", \"County\", \"State\", \"Timezone\", \"Wind_Direction\", \"Weather_Condition\"]\n",
    "\n",
    "# for column in list_strings:\n",
    "#   print(\"Hello\")\n",
    "#   counties = vectorize_data(train_df[column])\n",
    "#   train_df[column] = counties[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4DVQm1uHI5sD"
   },
   "outputs": [],
   "source": [
    "# def process_binary(column, class1, class2):\n",
    "#   train_df.loc[train_df[column] == class1, column] = 0\n",
    "#   train_df.loc[train_df[column] == class2, column] = 1\n",
    "\n",
    "list_binary = ['Amenity','Bump','Crossing','Give_Way','Junction','No_Exit','Railway','Roundabout','Station','Stop','Traffic_Calming','Traffic_Signal','Turning_Loop']\n",
    "\n",
    "#list_strings = [\"City\", \"County\", \"State\", \"Timezone\", \"Wind_Direction\", \"Weather_Condition\"]\n",
    "\n",
    "# for column in list_binary:\n",
    "#   print(\"Hello\")\n",
    "#   counties = vectorize_data(train_df[column])\n",
    "#   train_df[column] = counties[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dzP7H2SNR0tV"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRDEOSrSKZU8"
   },
   "outputs": [],
   "source": [
    "day_night_columns=['Sunrise_Sunset','Civil_Twilight','Nautical_Twilight','Astronomical_Twilight']\n",
    "# for column in day_night_columns:\n",
    "#   print(\"Hello\")\n",
    "#   counties = vectorize_data(train_df[column])\n",
    "#   train_df[column] = counties[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8EaL51z0Uich"
   },
   "outputs": [],
   "source": [
    "print(np.unique(np.array(train_df['Station'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AonO4AkVMgt"
   },
   "outputs": [],
   "source": [
    "1000 in train_df['Station']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VhC1SQE4Kwc7"
   },
   "outputs": [],
   "source": [
    "# column = 'Side'\n",
    "# counties = vectorize_data(train_df[column])\n",
    "#train_df[column] = counties[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YyyMNTM2LC1D"
   },
   "outputs": [],
   "source": [
    "# train_df.to_csv(\"All_numbers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n_7LWpWxLLXI"
   },
   "outputs": [],
   "source": [
    "train_df.drop('ID', axis=1, inplace=True)\n",
    "train_df.drop('Country', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dfrBz-jDB5U5"
   },
   "outputs": [],
   "source": [
    "most_nan_columns = ['Number','Wind_Chill(F)','Humidity(%)','Wind_Speed(mph)','Precipitation(in)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HuwtOXj_YGcp"
   },
   "outputs": [],
   "source": [
    "train_df.drop('Wind_Chill(F)', axis=1, inplace=True)\n",
    "train_df.drop('Humidity(%)', axis=1, inplace=True)\n",
    "train_df.drop('Wind_Speed(mph)', axis=1, inplace=True)\n",
    "train_df.drop('Precipitation(in)', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QfTkuiaMMyJ0"
   },
   "outputs": [],
   "source": [
    "y = train_df[\"Severity\"]\n",
    "y = y-1\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irv-kcdLL-L1"
   },
   "source": [
    "# Standardize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iJeUdyA0OZYG"
   },
   "outputs": [],
   "source": [
    "print(np.unique(train_df[\"Bump\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qcQsUYskXDmg"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "frames = copy.deepcopy(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cFy4S1qzMAUE"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "for column in train_df:\n",
    "  print(column)\n",
    "  scaler = MinMaxScaler()\n",
    "  data = np.array(train_df[column])\n",
    "  data = data.reshape(data.shape[0], 1)\n",
    "  scaler.fit(data)\n",
    "  #print(scaler.data_max_)\n",
    "  data = scaler.transform(data)\n",
    "  print(np.max(data))\n",
    "  train_df[column] = data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qBxXeUQCYXWT"
   },
   "source": [
    "# Train and val Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y5DAPYCyZCML"
   },
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbsBlzDHZGdt"
   },
   "outputs": [],
   "source": [
    "train_df.drop('Severity', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjCgEek3ZKyU"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_df, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpj_rg6KaY5Z"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p6rbCqpPZt5L"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(20, input_shape = (33,), activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(64, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(4, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FKX9E1oofHas"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "checkpoint_callbk = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"best_tiny_model\", # name of file to save the best model to\n",
    "    monitor=\"val_macroF1\", # prefix val to specify that we want the model with best macroF1 on the validation data\n",
    "    verbose=1, # prints out when the model achieve a better epoch\n",
    "    mode=\"max\", # the monitored metric should be maximized\n",
    "    save_freq=\"epoch\", # clear\n",
    "    save_best_only=True, # of course, if not, every time a new best is achieved will be savedf differently\n",
    "    save_weights_only=True # this means that we don't have to save the architecture, if you change the architecture, you'll loose the old weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6_sJ6TmvfqJS"
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow_addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "395l_G8KbNij"
   },
   "outputs": [],
   "source": [
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.00001)\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPKwwltjbj_8"
   },
   "outputs": [],
   "source": [
    "history=model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=128, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_7fGY1r0dS2z"
   },
   "outputs": [],
   "source": [
    "# test_value = np.array(X_test.iloc[1])\n",
    "# test_value = np.reshape(test_value, (1, 37))\n",
    "# model.predict(test_value)\n",
    "# # test_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orJaAOyZgnFk"
   },
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bRVEHlLtebE"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4m_7HU-tsBN"
   },
   "outputs": [],
   "source": [
    "print(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vc7CgzUvvx35"
   },
   "outputs": [],
   "source": [
    "print(predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MqxKiAS2t-xQ"
   },
   "outputs": [],
   "source": [
    "print(np.unique(predictions))\n",
    "print(predictions.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "inEKzcNzuiVd"
   },
   "outputs": [],
   "source": [
    "max_index = predictions.argmax(axis=1)\n",
    "print(max_index.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSODdKMzwL5k"
   },
   "outputs": [],
   "source": [
    "print(max_index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MRIiZHUNwgc_"
   },
   "outputs": [],
   "source": [
    "preds = np.add(max_index,np.ones(max_index.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qWpA5T6w_0M"
   },
   "outputs": [],
   "source": [
    "print(np.unique(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6-7MXgEdfFF"
   },
   "outputs": [],
   "source": [
    "# preds = []\n",
    "\n",
    "# print(len(X_test[\"State\"]))\n",
    "# for i in range(len(X_test[\"State\"])):\n",
    "#   if(i%10000 == 0):\n",
    "#     print(i)\n",
    "#   test_value = np.array(X_test.iloc[i])\n",
    "#   test_value = np.reshape(test_value, (1, 33))\n",
    "#   pred = np.argmax(model.predict(test_value)) + 1\n",
    "#   preds.append(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgouUrLpwojz"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation\n",
    "\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, preds))\n",
    "print(\"F1 score:\",metrics.f1_score(y_test, preds, average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dYvBoww0gzVY"
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(history.history[\"accuracy\"], label = \"Train accuracy\")\n",
    "plt.plot(history.history[\"loss\"], label = \"Train loss\")\n",
    "plt.plot(history.history[\"val_loss\"], label = \"Validation Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0GUf_izXIwjg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "orJaAOyZgnFk"
   ],
   "name": "Neural_net.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
